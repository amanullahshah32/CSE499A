{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4437a-80dc-4251-9d9c-a6c9e9bc53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "# Test PDF path - replace with a real PDF on your system\n",
    "pdf_path = \"Datasets/sample cv/scanned cv.pdf\"\n",
    "\n",
    "# Try with explicit poppler path\n",
    "poppler_path = r'C:\\Program Files\\poppler\\poppler-24.08.0\\Library\\bin'  # Adjust this to your installation path\n",
    "print(f\"Testing PDF conversion with Poppler at: {poppler_path}\")\n",
    "\n",
    "try:\n",
    "    # Convert first page only for testing\n",
    "    images = convert_from_path(pdf_path, poppler_path=poppler_path, first_page=1, last_page=1)\n",
    "    print(f\"Success! Converted 1 page from PDF to image.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f431a0e-be9e-47f4-b0f0-8a5b5c357c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## Advanced Resume Ranking System with XAI Feedback\n",
    "# Add these imports at the top of your file\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import random\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import docx\n",
    "import spacy\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Union, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('resume_ranker.log'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Force download and update NLTK resources\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('stopwords', force=True)\n",
    "nltk.download('wordnet', force=True)\n",
    "nltk.download('omw-1.4', force=True)\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    logging.error(\"spaCy English model not found. Run 'python -m spacy download en_core_web_sm'\")\n",
    "    raise\n",
    "\n",
    "# Simple fallback lemmatizer class\n",
    "class LegacyLemmatizer:\n",
    "    \"\"\"Fallback lemmatizer when NLTK's doesn't work\"\"\"\n",
    "    def lemmatize(self, word):\n",
    "        \"\"\"Simple lemmatization rules\"\"\"\n",
    "        if word.endswith('ing'):\n",
    "            return word[:-3]\n",
    "        if word.endswith('ed'):\n",
    "            return word[:-2]\n",
    "        if word.endswith('s') and not word.endswith('ss'):\n",
    "            return word[:-1]\n",
    "        return word\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Enhanced ResumeRanker Class with XAI Feedback Systems\n",
    "\n",
    "class ResumeRanker:\n",
    "    \"\"\"Advanced resume ranking system with bias mitigation and explainable feedback\"\"\"\n",
    "    \n",
    "    def __init__(self, job_description: str = None):\n",
    "        self.job_description = job_description\n",
    "        self.all_resumes = []\n",
    "        \n",
    "        # Initialize NLTK with error handling\n",
    "        self._init_nltk_resources()\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english') if 'stopwords' in nltk.data.path else []).union({\n",
    "            'resume', 'cv', 'references', 'available upon request', 'page'\n",
    "        })\n",
    "        \n",
    "        # Initialize NLP\n",
    "        try:\n",
    "            self.nlp = nlp\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing spaCy: {str(e)}\")\n",
    "            self.nlp = None\n",
    "\n",
    "        # initialize OCR capabilites\n",
    "        self._init_ocr_capabilities()\n",
    "        \n",
    "        \n",
    "    \n",
    "        # Rest of your initialization code...\n",
    "    def _init_ocr_capabilities(self):\n",
    "        \"\"\"Initialize and check OCR capabilities\"\"\"\n",
    "        # Check Tesseract\n",
    "        try:\n",
    "            import pytesseract\n",
    "            pytesseract.get_tesseract_version()\n",
    "            self.tesseract_available = True\n",
    "            logging.info(\"Tesseract OCR initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            self.tesseract_available = False\n",
    "            logging.warning(f\"Tesseract OCR not available: {str(e)}\")\n",
    "        \n",
    "        # Check EasyOCR availability (but don't initialize yet, as it's slow)\n",
    "        try:\n",
    "            import easyocr\n",
    "            self.easyocr_available = True\n",
    "            logging.info(\"EasyOCR is available.\")\n",
    "        except ImportError:\n",
    "            self.easyocr_available = False\n",
    "            logging.warning(\"EasyOCR not installed. Run 'pip install easyocr' to enable it.\")\n",
    "        \n",
    "        # Initialize OCR configuration\n",
    "        self.ocr_config = {\n",
    "            'use_tesseract': self.tesseract_available,\n",
    "            'use_easyocr': self.easyocr_available,\n",
    "            'use_cloud_ocr': False,  # Set to True when you have an API key\n",
    "            'cloud_ocr_api_key': 'YOUR_API_KEY',  # Replace with your actual key\n",
    "            'timeout': 120,\n",
    "            'poppler_path': r'C:\\Program Files\\poppler\\poppler-24.08.0\\Library\\bin'\n",
    "        }\n",
    "        \n",
    "        # Check poppler\n",
    "        try:\n",
    "            from pdf2image import convert_from_path\n",
    "            # Test with a sample file if available\n",
    "            sample_files = [f for f in os.listdir() if f.lower().endswith('.pdf')]\n",
    "            if sample_files:\n",
    "                convert_from_path(\n",
    "                    sample_files[0], \n",
    "                    poppler_path=self.ocr_config['poppler_path'],\n",
    "                    first_page=1, \n",
    "                    last_page=1\n",
    "                )\n",
    "                logging.info(\"PDF to image conversion working correctly.\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"PDF to image conversion may not work: {str(e)}\")\n",
    "\n",
    "        # Configuration\n",
    "        self.config = {\n",
    "            'scoring_weights': {\n",
    "                'education': 0.15,\n",
    "                'experience': 0.20,\n",
    "                'skills': 0.15,\n",
    "                'certifications': 0.10,\n",
    "                'projects': 0.10,\n",
    "                'jd_similarity': 0.30\n",
    "            },\n",
    "            'skill_threshold': 85,\n",
    "            'max_workers': 4,\n",
    "            'experience_patterns': [\n",
    "                r'(\\d+)\\+?\\s*(?:years?|yrs?)\\b.+?experience',\n",
    "                r'experience.*?(\\d+)\\+?\\s*(?:years?|yrs?)\\b',\n",
    "                r'\\b(\\d+\\+?\\s*(?:years?|yrs?))\\b.*?(experience|exp\\.?)'\n",
    "            ],\n",
    "            'hr_feedback_top_n': 50,\n",
    "            'feedback_min_rank': 5,\n",
    "            'feedback_max_rank': 20,\n",
    "            'benchmark_sample': 0.2,\n",
    "            'ocr': {\n",
    "                'language': 'eng',\n",
    "                'page_segmentation_mode': 1,  # Automatic page segmentation with OSD\n",
    "                'ocr_engine_mode': 3,         # Default, based on what is available\n",
    "                'timeout': 180,               # Maximum time in seconds\n",
    "                'preprocess': True            # Whether to apply image preprocessing\n",
    "            }\n",
    "        }\n",
    "\n",
    "                # Fix for NLTK initialization issues\n",
    "    def _init_nltk_resources(self):\n",
    "        \"\"\"Initialize NLTK resources with error handling\"\"\"\n",
    "        try:\n",
    "            # Force download essential NLTK resources\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            nltk.download('wordnet', quiet=True)\n",
    "            nltk.download('omw-1.4', quiet=True)\n",
    "            \n",
    "            # Prevent WordNetCorpusReader error by ensuring it's properly loaded\n",
    "            from nltk.corpus import wordnet\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "            # Test lemmatization to ensure it works\n",
    "            test_word = self.lemmatizer.lemmatize(\"testing\")\n",
    "            logging.info(\"NLTK resources initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"NLTK initialization error: {str(e)}\")\n",
    "            # Fallback to simple lemmatizer if WordNet fails\n",
    "            self.lemmatizer = LegacyLemmatizer()\n",
    "            logging.warning(\"Using fallback lemmatizer due to NLTK errors\")\n",
    "        \n",
    "        # # Validate Tesseract OCR installation\n",
    "        # try:\n",
    "        #     pytesseract.get_tesseract_version()\n",
    "        #     self.ocr_enabled = True\n",
    "        #     logging.info(\"Tesseract OCR initialized successfully.\")\n",
    "        # except Exception as e:\n",
    "        #     logging.warning(f\"Tesseract OCR not properly configured: {str(e)}. Scanned document processing will be limited.\")\n",
    "        #     self.ocr_enabled = False\n",
    "    \n",
    "        # Initialize the rest of your code...\n",
    "        self._init_feedback_templates()\n",
    "        # Rest of your initialization code...\n",
    "\n",
    "        # Enhanced skill matrix\n",
    "        self.skill_matrix = {\n",
    "            'programming': ['python', 'java', 'c++', 'javascript', 'sql', 'r', \n",
    "                           'html5', 'css3', 'react', 'node.js', 'angular', 'vue.js'],\n",
    "            'data_science': ['machine learning', 'deep learning', 'data analysis', \n",
    "                            'pandas', 'numpy', 'tensorflow', 'pytorch', 'nlp'],\n",
    "            'cloud': ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform'],\n",
    "            'databases': ['mysql', 'postgresql', 'mongodb', 'oracle', 'redis'],\n",
    "            'devops': ['ci/cd', 'jenkins', 'ansible', 'git', 'linux', 'bash'],\n",
    "            'design': ['ui/ux', 'figma', 'adobe xd', 'photoshop', 'sketch']\n",
    "        }\n",
    "\n",
    "        # Expanded education terms\n",
    "        self.education_terms = {\n",
    "            'bachelor': {\n",
    "                'score': 3,\n",
    "                'keywords': ['bachelor', 'bs', 'bsc', 'ba', 'b.tech', 'undergraduate'],\n",
    "                'degrees': ['bsc', 'ba', 'bcom', 'beng']\n",
    "            },\n",
    "            'master': {\n",
    "                'score': 4,\n",
    "                'keywords': ['master', 'ms', 'm.sc', 'mba', 'postgraduate'],\n",
    "                'degrees': ['msc', 'ma', 'mba', 'meng']\n",
    "            },\n",
    "            'phd': {\n",
    "                'score': 5,\n",
    "                'keywords': ['phd', 'doctorate', 'doctoral'],\n",
    "                'degrees': ['phd']\n",
    "            },\n",
    "            'diploma': {\n",
    "                'score': 2,\n",
    "                'keywords': ['diploma', 'associate', 'certificate'],\n",
    "                'degrees': ['diploma']\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Certification patterns\n",
    "        self.certifications = {\n",
    "            'aws': ['aws certified', 'amazon web services'],\n",
    "            'google': ['google cloud certified'],\n",
    "            'microsoft': ['microsoft certified'],\n",
    "            'pmp': ['project management professional'],\n",
    "            'scrum': ['scrum master', 'agile certified']\n",
    "        }\n",
    "    def _is_scanned_pdf(self, file_path: str) -> bool:\n",
    "        \"\"\"Enhanced scanned PDF detection using layout analysis\"\"\"\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text_content = ''\n",
    "                image_count = 0\n",
    "                \n",
    "                # Sample first 3 pages or all pages if less than 3\n",
    "                sample_pages = min(3, len(pdf.pages))\n",
    "                for i in range(sample_pages):\n",
    "                    page = pdf.pages[i]\n",
    "                    text = page.extract_text(x_tolerance=1, y_tolerance=1)\n",
    "                    text_content += text or ''\n",
    "                    \n",
    "                    # Check for image content\n",
    "                    if len(page.images) > 0:\n",
    "                        image_count += 1\n",
    "\n",
    "                # Decision logic\n",
    "                if len(text_content) < 500:  # Higher threshold\n",
    "                    if image_count > 0:\n",
    "                        return True\n",
    "                    return len(text_content) < 100  # Fallback threshold\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"PDF analysis error: {str(e)}\")\n",
    "            return True\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Function to check and fix dependencies\n",
    "    @staticmethod\n",
    "    def check_dependencies():\n",
    "        missing = []\n",
    "        \n",
    "        # Check Tesseract\n",
    "        try:\n",
    "            subprocess.run(['tesseract', '--version'], \n",
    "                        stdout=subprocess.PIPE, \n",
    "                        stderr=subprocess.PIPE,\n",
    "                        check=True)\n",
    "        except (FileNotFoundError, subprocess.CalledProcessError):\n",
    "            missing.append(\"Tesseract OCR\")\n",
    "            print(\"\"\"\n",
    "            Tesseract installation required:\n",
    "            - Windows: Download from UB Mannheim (https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "            - Mac: brew install tesseract\n",
    "            - Linux: sudo apt install tesseract-ocr\n",
    "            \"\"\")\n",
    "\n",
    "        # Check Poppler\n",
    "        try:\n",
    "            from pdf2image import pdfinfo_from_path\n",
    "            test_file = next((f for f in os.listdir() if f.endswith('.pdf')), None)\n",
    "            if test_file:\n",
    "                pdfinfo_from_path(test_file)\n",
    "        except Exception:\n",
    "            missing.append(\"Poppler\")\n",
    "            print(\"\"\"\n",
    "            Poppler installation required for PDF processing:\n",
    "            - Windows: Add poppler path to environment variables\n",
    "            - Mac: brew install poppler\n",
    "            - Linux: sudo apt-get install poppler-utils\n",
    "            \"\"\")\n",
    "\n",
    "        if missing:\n",
    "            print(f\"Critical missing dependencies: {', '.join(missing)}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    # Modified _extract_text_with_ocr method for better error handling\n",
    "    def _extract_text_with_ocr(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from scanned documents using multiple OCR methods with fallback chain\"\"\"\n",
    "        logging.info(f\"Processing scanned document with OCR: {file_path}\")\n",
    "        \n",
    "        # Method 1: Try Tesseract OCR first (with timeout handling)\n",
    "        try:\n",
    "            # Convert PDF to images with poppler\n",
    "            poppler_path = r'C:\\Program Files\\poppler\\poppler-24.08.0\\Library\\bin'\n",
    "            images = convert_from_path(\n",
    "                file_path,\n",
    "                poppler_path=poppler_path,\n",
    "                thread_count=2,\n",
    "                dpi=200\n",
    "            )\n",
    "            \n",
    "            # Process each page with Tesseract\n",
    "            extracted_text = []\n",
    "            for i, image in enumerate(images):\n",
    "                # Preprocess image for better OCR results\n",
    "                img_np = np.array(image)\n",
    "                gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "                _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "                pil_img = Image.fromarray(binary)\n",
    "                \n",
    "                # Extract text with Tesseract (with timeout)\n",
    "                try:\n",
    "                    text = pytesseract.image_to_string(\n",
    "                        pil_img, \n",
    "                        lang='eng',\n",
    "                        config='--psm 1 --oem 3'\n",
    "                    )\n",
    "                    extracted_text.append(text)\n",
    "                    logging.info(f\"Tesseract OCR completed for page {i+1}\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Tesseract OCR failed for page {i+1}: {str(e)}\")\n",
    "                    raise  # Re-raise to trigger fallback\n",
    "            \n",
    "            result = \"\\n\".join(extracted_text)\n",
    "            if len(result.strip()) > 50:  # Check if we got meaningful text\n",
    "                return result\n",
    "            else:\n",
    "                logging.warning(\"Tesseract OCR returned minimal text, trying EasyOCR\")\n",
    "                raise Exception(\"Insufficient text extracted with Tesseract\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"OCR failed: {str(e)}\")\n",
    "        \n",
    "        # Method 2: Try EasyOCR if Tesseract fails\n",
    "        logging.info(\"Falling back to EasyOCR\")\n",
    "        try:\n",
    "            easyocr_text = self._extract_text_with_easyocr(file_path)\n",
    "            if easyocr_text and len(easyocr_text.strip()) > 50:\n",
    "                logging.info(\"Successfully extracted text with EasyOCR\")\n",
    "                return easyocr_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EasyOCR fallback failed: {str(e)}\")\n",
    "        \n",
    "        # Method 3: Try Cloud OCR if EasyOCR fails\n",
    "        logging.info(\"Falling back to Cloud OCR\")\n",
    "        try:\n",
    "            cloud_ocr_text = self._extract_text_with_cloud_ocr(file_path)\n",
    "            if cloud_ocr_text and len(cloud_ocr_text.strip()) > 50:\n",
    "                logging.info(\"Successfully extracted text with Cloud OCR\")\n",
    "                return cloud_ocr_text\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Cloud OCR fallback failed: {str(e)}\")\n",
    "        \n",
    "        # If all OCR methods fail, use enhanced fallback\n",
    "        logging.warning(\"All OCR methods failed, using fallback text extraction\")\n",
    "        return self._fallback_text_extraction(file_path)\n",
    "    \n",
    "    # Add a fallback method for when OCR fails\n",
    "    def _fallback_text_extraction(self, file_path: str) -> str:\n",
    "        \"\"\"Enhanced fallback text extraction when all OCR methods fail\"\"\"\n",
    "        logging.info(f\"Using enhanced fallback extraction for {file_path}\")\n",
    "        \n",
    "        try:\n",
    "            # Try multiple extraction methods\n",
    "            extracted_text = \"\"\n",
    "            \n",
    "            # Method 1: Try pdfplumber with different parameters\n",
    "            try:\n",
    "                with pdfplumber.open(file_path) as pdf:\n",
    "                    laparams = {\n",
    "                        \"char_margin\": 10.0,\n",
    "                        \"line_margin\": 1.0,\n",
    "                        \"word_margin\": 0.1\n",
    "                    }\n",
    "                    text1 = \"\\n\".join([p.extract_text(laparams=laparams) or \"\" for p in pdf.pages])\n",
    "                    if len(text1) > 100:  # If substantial text was extracted\n",
    "                        extracted_text = text1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"pdfplumber extraction failed: {str(e)}\")\n",
    "                \n",
    "            # Method 2: Try PyPDF2 if pdfplumber failed\n",
    "            if not extracted_text:\n",
    "                try:\n",
    "                    import PyPDF2\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        reader = PyPDF2.PdfReader(file)\n",
    "                        text2 = \"\"\n",
    "                        for page in reader.pages:\n",
    "                            text2 += page.extract_text() or \"\"\n",
    "                        if len(text2) > 100:\n",
    "                            extracted_text = text2\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"PyPDF2 extraction failed: {str(e)}\")\n",
    "            \n",
    "            # If both methods failed, return a placeholder\n",
    "            if not extracted_text:\n",
    "                return f\"[SCANNED DOCUMENT: Text extraction failed for {os.path.basename(file_path)}]\"\n",
    "            \n",
    "            return extracted_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"All extraction methods failed: {str(e)}\")\n",
    "            return f\"[EXTRACTION FAILED: {os.path.basename(file_path)}]\"\n",
    "    def _init_feedback_templates(self):\n",
    "        \"\"\"Initialize natural language feedback templates with more variety and personalization options\"\"\"\n",
    "        self.feedback_templates = {\n",
    "            'hr_openers': [\n",
    "                \"This candidate stands out because...\",\n",
    "                \"Our analysis reveals...\",\n",
    "                \"Key strengths include...\",\n",
    "                \"Top ranking justified by...\",\n",
    "                \"This profile is particularly strong in...\",\n",
    "                \"The candidate demonstrates exceptional...\",\n",
    "                \"What makes this application notable is...\"\n",
    "            ],\n",
    "            'strength_connectors': {\n",
    "                'skills': [\n",
    "                    \"demonstrated expertise in\", \n",
    "                    \"proven capability with\",\n",
    "                    \"extensive experience using\",\n",
    "                    \"technical proficiency in\",\n",
    "                    \"mastery of\",\n",
    "                    \"specialized knowledge of\"\n",
    "                ],\n",
    "                'education': [\n",
    "                    \"advanced training in\",\n",
    "                    \"formal education focused on\",\n",
    "                    \"degree specialization aligning with\",\n",
    "                    \"academic background in\",\n",
    "                    \"educational qualifications in\"\n",
    "                ],\n",
    "                'experience': [\n",
    "                    \"proven track record of\",\n",
    "                    \"extensive experience in\",\n",
    "                    \"demonstrated success with\",\n",
    "                    \"professional history showing\",\n",
    "                    \"career progression in\"\n",
    "                ]\n",
    "            },\n",
    "            'comparative_phrases': [\n",
    "                \"exceeding the benchmark by {gap}\",\n",
    "                \"{gap} above the average\",\n",
    "                \"placing in the top {percentile} percentile\",\n",
    "                \"significantly outperforming peers in\",\n",
    "                \"standing out among applicants with\"\n",
    "            ],\n",
    "            'jobseeker_openers': [\n",
    "                \"Here are some targeted suggestions to strengthen your application:\",\n",
    "                \"To improve your candidacy for similar roles, consider:\",\n",
    "                \"Your profile could be enhanced by addressing these areas:\",\n",
    "                \"Based on our analysis, here are personalized recommendations:\",\n",
    "                \"To better align with this position's requirements, focus on:\"\n",
    "            ],\n",
    "            'improvement_suggestions': [\n",
    "                \"Consider developing skills in {missing_skills}\",\n",
    "                \"Highlight more quantitative achievements in past roles\",\n",
    "                \"Obtain certification in {suggested_certification}\",\n",
    "                \"Increase project documentation specificity\",\n",
    "                \"Strengthen your profile by demonstrating experience with {technology}\",\n",
    "                \"Emphasize your achievements related to {relevant_area}\",\n",
    "                \"Add metrics to showcase impact in previous roles\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## File Processing Utilities\n",
    "    \n",
    "    def _find_resume_files(self, root_folder: str) -> List[Dict]:\n",
    "        \"\"\"Recursively find all resume files with categories\"\"\"\n",
    "        resume_files = []\n",
    "        for root, _, files in os.walk(root_folder):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.pdf', '.docx', '.txt')):\n",
    "                    resume_files.append({\n",
    "                        'path': os.path.join(root, file),\n",
    "                        'category': os.path.basename(root)\n",
    "                    })\n",
    "        return resume_files\n",
    "\n",
    "    def _extract_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from various document types with OCR fallback for scanned documents\"\"\"\n",
    "        try:\n",
    "            if file_path.lower().endswith('.pdf'):\n",
    "                # Check if it's a scanned PDF\n",
    "                if self._is_scanned_pdf(file_path):\n",
    "                    logging.info(f\"Detected scanned PDF: {file_path}\")\n",
    "                    return self._extract_text_with_ocr(file_path)\n",
    "                else:\n",
    "                    # Use regular text extraction for normal PDFs\n",
    "                    with pdfplumber.open(file_path) as pdf:\n",
    "                        text = \"\\n\".join([page.extract_text(x_tolerance=1, y_tolerance=1) \n",
    "                                        for page in pdf.pages if page.extract_text()])\n",
    "                        if text.strip():\n",
    "                            return text\n",
    "                        else:\n",
    "                            # If no text was extracted, try OCR anyway\n",
    "                            logging.info(f\"PDF appears to be scanned or has no extractable text: {file_path}\")\n",
    "                            return self._extract_text_with_ocr(file_path)\n",
    "            elif file_path.lower().endswith('.docx'):\n",
    "                doc = docx.Document(file_path)\n",
    "                return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            elif file_path.lower().endswith('.txt'):\n",
    "                with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                    return f.read()\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process {file_path}: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _remove_pii(self, text: str) -> str:\n",
    "        \"\"\"Remove personally identifiable information using spaCy NER\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        redacted = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in ['PERSON', 'EMAIL', 'PHONE', 'GPE']:\n",
    "                redacted.append('[REDACTED]')\n",
    "            else:\n",
    "                redacted.append(ent.text)\n",
    "        return ' '.join(redacted)\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Advanced text normalization with lemmatization and PII removal\"\"\"\n",
    "        text = self._remove_pii(text)\n",
    "        exp_numbers = re.findall(r'\\d+\\+?\\s*(?:years?|yrs?)', text.lower())\n",
    "        \n",
    "        text = re.sub(r'[^\\w\\s+]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "        \n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        lemmatized = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                      if token not in self.stop_words and len(token) > 2]\n",
    "        \n",
    "        return ' '.join(lemmatized + exp_numbers)\n",
    "\n",
    "    def _extract_contact_email(self, text: str) -> str:\n",
    "        \"\"\"Extract the first found email address from the text using regex\"\"\"\n",
    "        email_pattern = r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+'\n",
    "        matches = re.findall(email_pattern, text)\n",
    "        return matches[0] if matches else ''\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## Enhanced Feature Extraction\n",
    "    \n",
    "    def _extract_education_details(self, text: str) -> Dict:\n",
    "        \"\"\"Improved education extraction with degree detection\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        education = {\n",
    "            'highest_degree': 'None',\n",
    "            'degrees': [],\n",
    "            'score': 0\n",
    "        }\n",
    "        \n",
    "        for chunk in doc.noun_chunks:\n",
    "            chunk_text = chunk.text.lower()\n",
    "            for degree, config in self.education_terms.items():\n",
    "                if any(fuzz.partial_ratio(kw, chunk_text) > 85 for kw in config['keywords']):\n",
    "                    education['degrees'].append(degree)\n",
    "                    if config['score'] > education['score']:\n",
    "                        education.update({\n",
    "                            'highest_degree': degree,\n",
    "                            'score': config['score']\n",
    "                        })\n",
    "        return education\n",
    "\n",
    "    def _extract_experience(self, text: str) -> Dict:\n",
    "        \"\"\"Advanced experience analysis using spaCy NER\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        experience = {\n",
    "            'total_years': 0,\n",
    "            'score': 0\n",
    "        }\n",
    "        \n",
    "        dates = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'DATE':\n",
    "                dates.append(ent.text)\n",
    "        \n",
    "        experience['total_years'] = self._parse_dates(dates)\n",
    "        experience['score'] = min(experience['total_years'], 15)\n",
    "        \n",
    "        return experience\n",
    "\n",
    "    def _parse_dates(self, dates: List[str]) -> int:\n",
    "        \"\"\"Convert date entities to total years experience\"\"\"\n",
    "        year_pattern = r'\\b(20\\d{2}|\\d{2})\\b'\n",
    "        years = []\n",
    "        for date_str in dates:\n",
    "            matches = re.findall(year_pattern, date_str)\n",
    "            if matches:\n",
    "                years.extend([int(y) if len(y) == 4 else 2000 + int(y) for y in matches])\n",
    "        \n",
    "        if len(years) >= 2:\n",
    "            return max(years) - min(years)\n",
    "        return 0\n",
    "\n",
    "    def _extract_skills(self, text: str) -> List[str]:\n",
    "        \"\"\"Hybrid skill extraction using fuzzy matching\"\"\"\n",
    "        detected_skills = []\n",
    "        flat_skills = [skill for cats in self.skill_matrix.values() for skill in cats]\n",
    "        \n",
    "        for skill in flat_skills:\n",
    "            if process.extractOne(skill, text.split(), \n",
    "                                scorer=fuzz.token_set_ratio)[1] > self.config['skill_threshold']:\n",
    "                detected_skills.append(skill)\n",
    "        \n",
    "        return list(set(detected_skills))\n",
    "\n",
    "    def _skill_score(self, text: str) -> int:\n",
    "        \"\"\"Calculate normalized skill score\"\"\"\n",
    "        detected_skills = self._extract_skills(text)\n",
    "        return min(len(detected_skills), 20)  # Cap at 20 skills\n",
    "\n",
    "    def _detect_certifications(self, text: str) -> List[str]:\n",
    "        \"\"\"Identify certifications in resume text\"\"\"\n",
    "        certs = set()\n",
    "        text_lower = text.lower()\n",
    "        for cert, keywords in self.certifications.items():\n",
    "            for kw in keywords:\n",
    "                if re.search(r'\\b' + re.escape(kw) + r'\\b', text_lower):\n",
    "                    certs.add(cert)\n",
    "                    break\n",
    "        return list(certs)\n",
    "\n",
    "    def _project_count(self, text: str) -> int:\n",
    "        \"\"\"Improved project detection with context analysis\"\"\"\n",
    "        project_keywords = r'\\bproject\\b|\\bportfolio\\b|\\bwork\\s+experience\\b|\\bselected\\s+works?\\b'\n",
    "        sections = re.split(project_keywords, text, flags=re.IGNORECASE)\n",
    "        return min(len(sections) - 1, 10)  # Subtract 1 for initial split\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## Parallel Processing Pipeline\n",
    "    \n",
    "    def process_resumes(self, dataset_path: str) -> None:\n",
    "        \"\"\"Parallel resume processing with ThreadPoolExecutor\"\"\"\n",
    "        self.all_resumes = []\n",
    "        files = self._find_resume_files(dataset_path)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:\n",
    "            futures = [executor.submit(self._process_single_resume, file_info) \n",
    "                      for file_info in files]\n",
    "            \n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    self.all_resumes.append(result)\n",
    "\n",
    "    def _process_single_resume(self, file_info: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Process individual resume with error handling\"\"\"\n",
    "        try:\n",
    "            raw_text = self._extract_text(file_info['path'])\n",
    "            if not raw_text.strip():\n",
    "                return None\n",
    "                \n",
    "            preprocessed = self.preprocess_text(raw_text)\n",
    "            education_details = self._extract_education_details(preprocessed)\n",
    "            experience_details = self._extract_experience(raw_text)\n",
    "            contact_email = self._extract_contact_email(raw_text)\n",
    "            \n",
    "            return {\n",
    "                'file_name': os.path.basename(file_info['path']),\n",
    "                'job_category': file_info['category'],\n",
    "                'contact_email': contact_email,\n",
    "                'education': education_details['highest_degree'],\n",
    "                'education_score': education_details['score'],\n",
    "                'experience_score': experience_details['score'],\n",
    "                'experience_years': experience_details['total_years'],\n",
    "                'detected_skills': self._extract_skills(preprocessed),\n",
    "                'certifications': self._detect_certifications(raw_text),\n",
    "                'projects': self._project_count(raw_text),\n",
    "                'jd_similarity': self._calculate_jd_similarity(preprocessed),\n",
    "                'skill_score': self._skill_score(preprocessed),\n",
    "                'total_score': 0,  # Will be calculated later\n",
    "                'hr_feedback': '',\n",
    "                'improvement_areas': ''\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {file_info['path']}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## Enhanced Scoring System\n",
    "    \n",
    "    def _calculate_jd_similarity(self, text: str) -> float:\n",
    "        \"\"\"Cached TF-IDF similarity calculation\"\"\"\n",
    "        if not hasattr(self, '_jd_vector'):\n",
    "            vectorizer = TfidfVectorizer()\n",
    "            jd_clean = self.preprocess_text(self.job_description)\n",
    "            self._jd_vector = vectorizer.fit_transform([jd_clean])\n",
    "            self._vectorizer = vectorizer\n",
    "        \n",
    "        resume_vector = self._vectorizer.transform([text])\n",
    "        return cosine_similarity(self._jd_vector, resume_vector)[0][0]\n",
    "\n",
    "    def _calculate_scores(self) -> None:\n",
    "        \"\"\"Calculate final scores for all resumes\"\"\"\n",
    "        for resume in self.all_resumes:\n",
    "            scores = {\n",
    "                'education': resume['education_score'],\n",
    "                'experience': resume['experience_score'],\n",
    "                'skills': resume['skill_score'],\n",
    "                'certifications': len(resume['certifications']) * 2,\n",
    "                'projects': resume['projects'],\n",
    "                'jd_similarity': resume['jd_similarity']\n",
    "            }\n",
    "            \n",
    "            weights = self.config['scoring_weights']\n",
    "            resume['total_score'] = sum(scores[cat] * weights[cat] for cat in weights)\n",
    "\n",
    "    # %% [markdown]\n",
    "    # ## XAI Feedback Generation System\n",
    "    \n",
    "    def generate_feedback(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate explainable feedback for HR and candidates\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "    \n",
    "        # Calculate benchmarks\n",
    "        top_candidates = df[df['rank'] <= max(10, int(len(df)*self.config['benchmark_sample']))]\n",
    "        benchmarks = {\n",
    "            'skills': top_candidates['skill_score'].quantile(0.75),\n",
    "            'experience': top_candidates['experience_years'].median(),\n",
    "            'education': top_candidates['education_score'].max(),\n",
    "            'jd_similarity': top_candidates['jd_similarity'].mean()\n",
    "        }\n",
    "    \n",
    "        # Generate HR feedback\n",
    "        df['hr_feedback'] = df.apply(\n",
    "            lambda row: self._generate_hr_feedback(row, benchmarks) \n",
    "            if row['rank'] <= self.config['hr_feedback_top_n'] else '', \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        # Generate job seeker feedback\n",
    "        df['job_seeker_feedback'] = df.apply(\n",
    "            lambda row: self._identify_improvement_areas(row, benchmarks)\n",
    "            if self.config['feedback_min_rank'] <= row['rank'] <= self.config['feedback_max_rank'] else '',\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "        return df\n",
    "\n",
    "    def _generate_hr_feedback(self, candidate: pd.Series, benchmarks: dict) -> str:\n",
    "        \"\"\"Generate detailed natural language feedback for HR with specific candidate insights\"\"\"\n",
    "        # Identify top strengths\n",
    "        strengths = self._identify_key_strengths(candidate, benchmarks)\n",
    "        \n",
    "        # Build feedback sentence\n",
    "        opener = random.choice(self.feedback_templates['hr_openers'])\n",
    "        \n",
    "        # Create detailed strength descriptions with comparative elements\n",
    "        strength_phrases = []\n",
    "        for stype, values in strengths.items():\n",
    "            if values:\n",
    "                connector = random.choice(self.feedback_templates['strength_connectors'][stype])\n",
    "                values_str = ', '.join(values)\n",
    "                \n",
    "                # Add comparative element if applicable\n",
    "                if stype == 'skills' and candidate['skill_score'] > benchmarks['skills']:\n",
    "                    gap = f\"{(candidate['skill_score'] - benchmarks['skills']):.1f} points\"\n",
    "                    percentile = random.randint(85, 95)\n",
    "                    comparative = random.choice(self.feedback_templates['comparative_phrases'])\n",
    "                    comparative = comparative.format(gap=gap, percentile=percentile)\n",
    "                    strength_phrases.append(f\"{connector} {values_str} ({comparative})\")\n",
    "                elif stype == 'experience' and candidate['experience_years'] > benchmarks['experience']:\n",
    "                    gap = f\"{(candidate['experience_years'] - benchmarks['experience']):.1f} years\"\n",
    "                    comparative = random.choice(self.feedback_templates['comparative_phrases'])\n",
    "                    comparative = comparative.format(gap=gap, percentile=\"N/A\")\n",
    "                    strength_phrases.append(f\"{connector} {values_str} ({comparative})\")\n",
    "                else:\n",
    "                    strength_phrases.append(f\"{connector} {values_str}\")\n",
    "        \n",
    "        # Add differentiators\n",
    "        differentiators = self._identify_differentiators(candidate)\n",
    "        if differentiators:\n",
    "            strength_phrases.append(f\"Notable differentiators: {differentiators}\")\n",
    "        \n",
    "        # Add JD relevance if it's high\n",
    "        if candidate['jd_similarity'] > 0.7:\n",
    "            jd_match = f\"{candidate['jd_similarity']*100:.1f}%\"\n",
    "            strength_phrases.append(f\"Exceptional job description match of {jd_match}\")\n",
    "            \n",
    "        # Generate final feedback\n",
    "        feedback = f\"{opener} {'. '.join(strength_phrases)}.\"\n",
    "        \n",
    "        # Add specific recommendation for this candidate if applicable\n",
    "        if candidate['certifications']:\n",
    "            cert_str = ', '.join(candidate['certifications'][:2])\n",
    "            feedback += f\" Particularly valuable are the {cert_str} certifications which align with our technology stack.\"\n",
    "        \n",
    "        return feedback\n",
    "\n",
    "    def _identify_key_strengths(self, candidate: pd.Series, benchmarks: dict) -> dict:\n",
    "        \"\"\"Identify candidate's standout features\"\"\"\n",
    "        strengths = {}\n",
    "        \n",
    "        # Skill strength\n",
    "        if candidate['skill_score'] > benchmarks['skills']:\n",
    "            top_skills = candidate['detected_skills'][:3]\n",
    "            strengths['skills'] = [f\"{s} ({self._get_skill_context(s)})\" for s in top_skills]\n",
    "            \n",
    "        # Experience strength\n",
    "        if candidate['experience_years'] > benchmarks['experience']:\n",
    "            exp_strength = f\"{candidate['experience_years']} years (vs avg {benchmarks['experience']:.1f})\"\n",
    "            strengths['experience'] = [exp_strength]\n",
    "            \n",
    "        return strengths\n",
    "\n",
    "    def _get_skill_context(self, skill: str) -> str:\n",
    "        \"\"\"Add contextual description for skills\"\"\"\n",
    "        contexts = {\n",
    "            'python': \"Python development\",\n",
    "            'aws': \"cloud infrastructure\",\n",
    "            'machine learning': \"predictive modeling\",\n",
    "            'react': \"frontend development\"\n",
    "        }\n",
    "        return contexts.get(skill.lower(), \"relevant technical area\")\n",
    "\n",
    "    def _identify_differentiators(self, candidate: pd.Series) -> str:\n",
    "        \"\"\"Find unique candidate differentiators\"\"\"\n",
    "        diffs = []\n",
    "        \n",
    "        # Certification differentiator\n",
    "        if candidate['certifications']:\n",
    "            diffs.append(f\"Certifications: {', '.join(candidate['certifications'])}\")\n",
    "            \n",
    "        # Project differentiator\n",
    "        if candidate['projects'] > 5:\n",
    "            diffs.append(f\"Substantial project portfolio ({candidate['projects']} documented)\")\n",
    "            \n",
    "        return '; '.join(diffs) if diffs else ''\n",
    "\n",
    "# These methods need to be properly indented within the ResumeRanker class definition\n",
    "# The code below should replace the incorrectly indented module-level functions\n",
    "\n",
    "    def _get_skill_importance_context(self, skill: str) -> str:\n",
    "        \"\"\"Explain why a particular skill is important\"\"\"\n",
    "        contexts = {\n",
    "            'python': \"essential for data processing and backend development\",\n",
    "            'react': \"increasingly in demand for modern web applications\",\n",
    "            'aws': \"critical for cloud-native application development\",\n",
    "            'kubernetes': \"valuable for containerized application orchestration\",\n",
    "            'machine learning': \"growing area for predictive analytics solutions\",\n",
    "            'ci/cd': \"key for modern software delivery practices\"\n",
    "        }\n",
    "        return contexts.get(skill.lower(), \"highly valued in current job market\")\n",
    "\n",
    "    def _suggest_relevant_certification(self, skills: List[str]) -> str:\n",
    "        \"\"\"Suggest certification based on candidate's existing skills\"\"\"\n",
    "        skill_to_cert = {\n",
    "            'python': \"Python Professional\",\n",
    "            'java': \"Oracle Java\",\n",
    "            'javascript': \"JavaScript Fullstack\",\n",
    "            'react': \"React Developer\",\n",
    "            'aws': \"AWS Solutions Architect\",\n",
    "            'azure': \"Azure Developer\",\n",
    "            'kubernetes': \"CKA (Certified Kubernetes Administrator)\",\n",
    "            'docker': \"Docker Certified Associate\",\n",
    "            'machine learning': \"TensorFlow Developer\"\n",
    "        }\n",
    "        \n",
    "        # Find matching certification based on skills\n",
    "        for skill in skills:\n",
    "            if skill.lower() in skill_to_cert:\n",
    "                return skill_to_cert[skill.lower()]\n",
    "        \n",
    "        # Default certifications if no match\n",
    "        return random.choice([\"AWS Cloud Practitioner\", \"Scrum Master\", \"CompTIA A+\"])\n",
    "\n",
    "    def _identify_improvement_areas(self, candidate: pd.Series, benchmarks: dict) -> str:\n",
    "        \"\"\"Generate detailed personalized improvement suggestions with actionable insights\"\"\"\n",
    "        opener = random.choice(self.feedback_templates['jobseeker_openers'])\n",
    "        gaps = []\n",
    "        \n",
    "        # Skill gaps analysis\n",
    "        missing_skills = self._get_missing_skills(candidate)\n",
    "        if missing_skills:\n",
    "            skills_str = ', '.join(missing_skills[:3])\n",
    "            skill_suggestion = f\"Develop skills in: {skills_str}\"\n",
    "            \n",
    "            # Add specific context why these skills matter\n",
    "            context = self._get_skill_importance_context(missing_skills[0]) if missing_skills else \"\"\n",
    "            if context:\n",
    "                skill_suggestion += f\" ({context})\"\n",
    "            gaps.append(skill_suggestion)\n",
    "        \n",
    "        # Experience gaps with specific recommendations\n",
    "        if candidate['experience_years'] < benchmarks['experience']:\n",
    "            gap = benchmarks['experience'] - candidate['experience_years']\n",
    "            exp_suggestion = f\"Gain {gap:.1f} more years of relevant experience\"\n",
    "            \n",
    "            # Add specific advice\n",
    "            if candidate['experience_years'] > 0:\n",
    "                exp_suggestion += \" by seeking roles with greater responsibility or project leadership\"\n",
    "            else:\n",
    "                exp_suggestion += \" through internships, freelance work, or open-source contributions\"\n",
    "            gaps.append(exp_suggestion)\n",
    "        \n",
    "        # Certification gaps with personalized recommendations\n",
    "        if not candidate['certifications']:\n",
    "            # Choose certification based on candidate's existing skills\n",
    "            suggested = self._suggest_relevant_certification(candidate['detected_skills'])\n",
    "            cert_suggestion = f\"Consider {suggested} certification to validate expertise\"\n",
    "            gaps.append(cert_suggestion)\n",
    "        \n",
    "        # Project portfolio improvement\n",
    "        if candidate['projects'] < 3:\n",
    "            gaps.append(\"Showcase more projects with quantifiable results and technical details\")\n",
    "        \n",
    "        # JD alignment suggestion\n",
    "        if candidate['jd_similarity'] < 0.6:\n",
    "            gaps.append(\"Align resume keywords more closely with job descriptions in your target role\")\n",
    "        \n",
    "        # Format the final feedback\n",
    "        full_feedback = f\"{opener} {' '.join(gaps)}\"\n",
    "        \n",
    "        # Add a personalized closing statement\n",
    "        if gaps:\n",
    "            full_feedback += \" These targeted improvements could significantly strengthen your competitiveness for similar positions.\"\n",
    "        else:\n",
    "            full_feedback = \"Your profile is strong across key areas. Consider highlighting quantitative achievements to further strengthen your application.\"\n",
    "        \n",
    "        return full_feedback\n",
    "\n",
    "    def _get_missing_skills(self, candidate: pd.Series) -> list:\n",
    "        \"\"\"Identify skills present in top candidates but missing\"\"\"\n",
    "        top_skills = set()\n",
    "        top_resumes = self.all_resumes[:int(len(self.all_resumes)*self.config['benchmark_sample'])]\n",
    "        for resume in top_resumes:\n",
    "            top_skills.update(resume['detected_skills'])\n",
    "                \n",
    "        return list(top_skills - set(candidate['detected_skills']))\n",
    "\n",
    "    def generate_jobseeker_feedback(self, output_dir: str = 'candidate_feedback'):\n",
    "        \"\"\"Generate personalized feedback files for candidates using ranked DataFrame\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        df = self.get_ranked_results()  # Use the ranked results which have the 'rank' key\n",
    "        # Filter candidates based on their rank\n",
    "        targets = df[(df['rank'] >= self.config['feedback_min_rank']) & \n",
    "                     (df['rank'] <= self.config['feedback_max_rank'])]\n",
    "        \n",
    "        for _, candidate in targets.iterrows():\n",
    "            filename = f\"{candidate['contact_email']}_feedback.txt\" if candidate['contact_email'] else f\"{candidate['file_name']}_feedback.txt\"\n",
    "            filepath = os.path.join(output_dir, filename)\n",
    "            \n",
    "            # Use the job_seeker_feedback column to create feedback content\n",
    "            feedback_content = self._format_jobseeker_feedback(candidate)\n",
    "            \n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(feedback_content)\n",
    "                logging.info(f\"Generated feedback for {filename}\")\n",
    "\n",
    "    # add the new OCR methods: \n",
    "    def _extract_text_with_easyocr(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from scanned documents using EasyOCR\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Processing scanned document with EasyOCR: {file_path}\")\n",
    "            \n",
    "            # Initialize EasyOCR reader (only do this once)\n",
    "            if not hasattr(self, 'easyocr_reader'):\n",
    "                import easyocr\n",
    "                logging.info(\"Initializing EasyOCR reader (first use)...\")\n",
    "                self.easyocr_reader = easyocr.Reader(['en'], gpu=False)\n",
    "            \n",
    "            # Convert PDF to images\n",
    "            poppler_path = r'C:\\Program Files\\poppler\\poppler-24.08.0\\Library\\bin'\n",
    "            images = convert_from_path(file_path, poppler_path=poppler_path, dpi=200)\n",
    "            \n",
    "            # Process each page\n",
    "            extracted_text = []\n",
    "            for i, image in enumerate(images):\n",
    "                # Convert PIL Image to numpy array\n",
    "                img_np = np.array(image)\n",
    "                \n",
    "                # Run EasyOCR\n",
    "                results = self.easyocr_reader.readtext(img_np)\n",
    "                \n",
    "                # Extract text from results\n",
    "                page_text = ' '.join([result[1] for result in results])\n",
    "                extracted_text.append(page_text)\n",
    "                \n",
    "                logging.info(f\"EasyOCR completed for page {i+1} of {file_path}\")\n",
    "            \n",
    "            return \"\\n\".join(extracted_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EasyOCR processing failed for {file_path}: {str(e)}\")\n",
    "            return \"\"  # Return empty string to trigger next OCR method\n",
    "    \n",
    "    def _extract_text_with_cloud_ocr(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text using OCR.space API\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Processing with Cloud OCR: {file_path}\")\n",
    "            \n",
    "            # Convert PDF to images\n",
    "            poppler_path = r'C:\\Program Files\\poppler\\poppler-24.08.0\\Library\\bin'\n",
    "            images = convert_from_path(file_path, poppler_path=poppler_path, dpi=200)\n",
    "            \n",
    "            # Process each page\n",
    "            extracted_text = []\n",
    "            for i, image in enumerate(images):\n",
    "                # Save image temporarily\n",
    "                temp_img_path = f\"temp_ocr_page_{i}.png\"\n",
    "                image.save(temp_img_path)\n",
    "                \n",
    "                # API configuration - get a free key from https://ocr.space/ocrapi\n",
    "                api_key = 'YOUR_API_KEY'  # Replace with your actual key\n",
    "                payload = {\n",
    "                    'apikey': api_key,\n",
    "                    'language': 'eng',\n",
    "                    'isOverlayRequired': False,\n",
    "                    'detectOrientation': True\n",
    "                }\n",
    "                \n",
    "                with open(temp_img_path, 'rb') as f:\n",
    "                    r = requests.post(\n",
    "                        'https://api.ocr.space/parse/image',\n",
    "                        files={temp_img_path: f},\n",
    "                        data=payload\n",
    "                    )\n",
    "                \n",
    "                # Clean up temp file\n",
    "                os.remove(temp_img_path)\n",
    "                \n",
    "                # Extract results\n",
    "                result = r.json()\n",
    "                if result.get('ParsedResults'):\n",
    "                    page_text = result['ParsedResults'][0]['ParsedText']\n",
    "                    extracted_text.append(page_text)\n",
    "                \n",
    "                logging.info(f\"Cloud OCR completed for page {i+1}\")\n",
    "            \n",
    "            return \"\\n\".join(extracted_text)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Cloud OCR failed: {str(e)}\")\n",
    "            return \"\"  # Return empty string to trigger fallback\n",
    "        \n",
    "    def _format_jobseeker_feedback(self, candidate: dict) -> str:\n",
    "        \"\"\"Format personalized feedback document using job_seeker_feedback content\"\"\"\n",
    "        return f\"\"\"\n",
    "        Dear Candidate,\n",
    "        \n",
    "        Thank you for your application. Here's personalized feedback to help strengthen your profile:\n",
    "        \n",
    "        {candidate['job_seeker_feedback']}\n",
    "        \n",
    "        Key Strengths:\n",
    "        - {random.choice(self._get_strengths_list(candidate))}\n",
    "        \n",
    "        Best regards,\n",
    "        HR Analytics Team\n",
    "        \"\"\"\n",
    "        \n",
    "    def _get_strengths_list(self, candidate: dict) -> list:\n",
    "        \"\"\"Identify candidate strengths for feedback\"\"\"\n",
    "        strengths = []\n",
    "        if candidate['skill_score'] > 0.6 * self.config['scoring_weights']['skills']:\n",
    "            strengths.append(f\"Strong technical skills in {', '.join(candidate['detected_skills'][:3])}\")\n",
    "        if candidate['projects'] > 3:\n",
    "            strengths.append(f\"Rich project experience ({candidate['projects']} documented)\")\n",
    "        return strengths if strengths else [\"Solid foundational qualifications\"]\n",
    "\n",
    "    def _get_recommendations(self, candidate: dict) -> str:\n",
    "        \"\"\"Generate actionable recommendations\"\"\"\n",
    "        recs = []\n",
    "        if len(candidate['certifications']) < 2:\n",
    "            recs.append(f\"Consider {random.choice(list(self.certifications.keys()))} certification\")\n",
    "        if candidate['jd_similarity'] < 0.6:\n",
    "            recs.append(\"Tailor resume keywords to better match job descriptions\")\n",
    "        if not recs:\n",
    "            recs.append(\"Enhance quantitative achievements in role descriptions\")\n",
    "        return '\\n'.join(f\"- {r}\" for r in recs)\n",
    "\n",
    "    def get_ranked_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate final ranked dataframe with feedback\"\"\"\n",
    "        self._calculate_scores()\n",
    "        df = pd.DataFrame(self.all_resumes)\n",
    "        \n",
    "        if not df.empty:\n",
    "            # Calculate ranks\n",
    "            df['rank'] = df['total_score'].rank(ascending=False, method='min').astype(int)\n",
    "            df = df.sort_values('rank')\n",
    "            \n",
    "            # Generate feedback\n",
    "            df = self.generate_feedback(df)\n",
    "            \n",
    "            # Update self.all_resumes with the rank field\n",
    "            self.all_resumes = df.to_dict(orient='records')\n",
    "            \n",
    "            # Reorder columns\n",
    "            cols = [\n",
    "                'rank', 'file_name', 'job_category', 'contact_email',\n",
    "                'education', 'education_score', 'experience_score', 'experience_years',\n",
    "                'detected_skills', 'certifications', 'projects',\n",
    "                'jd_similarity', 'skill_score', 'total_score',\n",
    "                'hr_feedback', 'job_seeker_feedback'  # Updated to include job_seeker_feedback\n",
    "            ]\n",
    "            return df[cols].reset_index(drop=True)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Execution Example with Feedback Generation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check dependencies first\n",
    "        missing_deps = ResumeRanker.check_dependencies()\n",
    "        if missing_deps:\n",
    "            print(\"Some dependencies are missing but we'll try to continue with limited functionality\")\n",
    "            \n",
    "        jd = \"\"\"Software Engineer with 3+ years experience in Python and cloud technologies\"\"\"\n",
    "        ranker = ResumeRanker(jd)\n",
    "        ranker.process_resumes(\"Datasets\")\n",
    "        \n",
    "        results = ranker.get_ranked_results()\n",
    "        if not results.empty:\n",
    "            # Save ranked results\n",
    "            results.to_csv(\"enhanced_rankings.csv\", index=False)\n",
    "           # print(\"\\nTop 5 Candidates with HR Feedback:\")\n",
    "           # print(results[['rank', 'file_name', 'hr_feedback']].head(5))\n",
    "            \n",
    "            # Generate candidate feedback files\n",
    "            ranker.generate_jobseeker_feedback()\n",
    "            print(\"\\nGenerated candidate feedback files in 'candidate_feedback' directory\")\n",
    "        else:\n",
    "            print(\"No resumes processed successfully\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.critical(f\"Fatal error: {str(e)}\", exc_info=True)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Feedback Analysis Cell (Jupyter-specific)\n",
    "\n",
    "def analyze_feedback():\n",
    "    \"\"\"Jupyter helper for feedback analysis\"\"\"\n",
    "    df = pd.read_csv(\"enhanced_rankings.csv\")\n",
    "    \n",
    "    print(\"HR Feedback Samples:\")\n",
    "    display(df[df['hr_feedback'] != ''][['file_name', 'hr_feedback']].head(5))\n",
    "    \n",
    "    print(\"\\nCommon Improvement Areas:\")\n",
    "    improvement_counts = df[df['improvement_areas'] != '']['improvement_areas'].value_counts()\n",
    "    display(improvement_counts.head(1))\n",
    "\n",
    "# Usage in Jupyter:\n",
    "# analyze_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f349a-2aa3-4365-a542-9c2bb0546579",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"enhanced_rankings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08689649-b9da-4b8f-8e60-78fe97d1354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "print(\"Poppler found and working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ae932-fb21-4361-8bd1-4b959087ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add at the beginning of your script\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37719a-1273-45b5-8af7-d580cfc6e218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a8df9-97c3-4ffa-80bf-1728f139abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,14]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e770e-3959-40aa-9077-91e905a970ef",
   "metadata": {},
   "source": [
    "## to see job seeker feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298799b6-0c1a-4142-b5c1-6ebce3f0ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this after main execution\n",
    "if os.path.exists(\"candidate_feedback\"):\n",
    "    sample_file = next(os.walk(\"candidate_feedback\"))[2][0]\n",
    "    with open(os.path.join(\"candidate_feedback\", sample_file), 'r') as f:\n",
    "        print(\"\\nSample Feedback Content:\")\n",
    "        print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

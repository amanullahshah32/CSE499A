{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650aab93-45a9-4086-b8ca-0509860a93cd",
   "metadata": {},
   "source": [
    "# Word Embedding from ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e068ee5-05fc-41ed-a505-887bfb7f1352",
   "metadata": {},
   "source": [
    "Project Overview and Setup\n",
    "This project aims to reduce recruitment bias by automatically parsing resumes (PDF and DOCX), ranking them using both traditional TF‑IDF and semantic similarity (Word2Vec), mitigating bias by removing demographic indicators, and providing detailed feedback for both recruiters and job seekers.\n",
    "\n",
    "Features:\n",
    "\n",
    "Improved Resume Parsing: Use advanced libraries (pdfplumber for PDFs, python‑docx with regex cleanup for DOCX) to extract text with better formatting.\n",
    "Word Embeddings (Word2Vec): Train a Word2Vec model to capture semantic meaning in resumes and job descriptions.\n",
    "Bias Mitigation: Detect and remove demographic indicators (like names and addresses) using spaCy’s NER.\n",
    "Enhanced Feedback System: Generate detailed feedback – explaining ranking rationale for recruiters and improvement suggestions for candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb5f48e-e562-47ac-b56d-14e1921be71e",
   "metadata": {},
   "source": [
    "# 1.  Improved Resume Parsing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470f58f2-dcab-44b6-8be5-0926308dc0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for text extraction and regex cleanup\n",
    "import re\n",
    "import pdfplumber   # Advanced PDF extraction library\n",
    "from docx import Document\n",
    "\n",
    "def extract_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Extracts and cleans text from a PDF or DOCX file.\n",
    "    For PDFs, uses pdfplumber to capture formatted text.\n",
    "    For DOCX, uses python-docx and regex for cleanup.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Path to the resume file.\n",
    "    \n",
    "    Returns:\n",
    "        text (str): The extracted and cleaned text.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        try:\n",
    "            # Open the PDF with pdfplumber and extract text from each page.\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text() or \"\"\n",
    "                    text += page_text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing PDF {file_path}: {e}\")\n",
    "    elif file_path.endswith(\".docx\"):\n",
    "        try:\n",
    "            # Open DOCX file and extract paragraphs.\n",
    "            doc = Document(file_path)\n",
    "            # Join paragraphs and apply regex cleanup to remove extra spaces/newlines.\n",
    "            raw_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "            # Regex cleanup: remove multiple spaces and trim whitespace.\n",
    "            text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DOCX {file_path}: {e}\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43859c-d7aa-4d25-9dba-49444c9327b4",
   "metadata": {},
   "source": [
    "# 2: Reading Resumes from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25c3090f-7681-4402-bda7-280f7aa36e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files found: 2484\n",
      "Total resumes processed: 2484\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_resumes_from_folder(root_folder):\n",
    "    \"\"\"\n",
    "    Recursively reads all PDF and DOCX files from a root folder and its subfolders.\n",
    "    \n",
    "    Parameters:\n",
    "        root_folder (str): The directory containing resume files.\n",
    "    \n",
    "    Returns:\n",
    "        resumes (list): List of extracted resume texts.\n",
    "    \"\"\"\n",
    "    resumes = []\n",
    "    total_files = 0\n",
    "    # Walk through directory and subdirectories\n",
    "    for dirpath, _, filenames in os.walk(root_folder):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".pdf\") or filename.endswith(\".docx\"):\n",
    "                total_files += 1\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    text = extract_text_from_file(file_path)\n",
    "                    resumes.append(text)  # Append the extracted text\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "    print(f\"Total files found: {total_files}\")\n",
    "    print(f\"Total resumes processed: {len(resumes)}\")\n",
    "    return resumes\n",
    "\n",
    "# Example usage: Change 'your_resume_directory' to your actual resume folder path.\n",
    "dataset_path = \"Datasets/data\"  # Replace with your actual folder path\n",
    "all_resumes = read_resumes_from_folder(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31488148-9cef-4bae-932c-17a34a46a5b6",
   "metadata": {},
   "source": [
    "# 3: Training the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4728e3-7505-4b82-8fee-116034f478f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model trained on resume data.\n"
     ]
    }
   ],
   "source": [
    "# Import gensim for Word2Vec training and a simple preprocessor for tokenization\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of texts into lists of words using gensim's simple_preprocess.\n",
    "    \n",
    "    Parameters:\n",
    "        texts (list): List of string texts.\n",
    "    \n",
    "    Returns:\n",
    "        tokenized_texts (list): List of lists, where each sublist contains tokens.\n",
    "    \"\"\"\n",
    "    return [simple_preprocess(text) for text in texts]\n",
    "\n",
    "# Tokenize all resume texts\n",
    "tokenized_resumes = tokenize_texts(all_resumes)\n",
    "\n",
    "# Train the Word2Vec model on the tokenized resumes\n",
    "# Parameters: size=100 dimensions, window=5 for context, min_count=2 to ignore rare words, and workers for parallelism.\n",
    "w2v_model = Word2Vec(sentences=tokenized_resumes, vector_size=100, window=5, min_count=2, workers=4)\n",
    "print(\"Word2Vec model trained on resume data.\")\n",
    "\n",
    "# Optional: Function to compute average Word2Vec similarity between two texts\n",
    "import numpy as np\n",
    "\n",
    "def compute_avg_w2v_similarity(text1, text2, model):\n",
    "    \"\"\"\n",
    "    Computes the average cosine similarity between the word vectors of two texts.\n",
    "    \n",
    "    Parameters:\n",
    "        text1 (str): First text.\n",
    "        text2 (str): Second text.\n",
    "        model (Word2Vec): Trained Word2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "        avg_similarity (float): Average cosine similarity between words of the two texts.\n",
    "    \"\"\"\n",
    "    tokens1 = simple_preprocess(text1)\n",
    "    tokens2 = simple_preprocess(text2)\n",
    "    # Filter tokens that are in the model's vocabulary.\n",
    "    tokens1 = [token for token in tokens1 if token in model.wv]\n",
    "    tokens2 = [token for token in tokens2 if token in model.wv]\n",
    "    if not tokens1 or not tokens2:\n",
    "        return 0.0\n",
    "    similarities = []\n",
    "    # Compute similarity for each pair and average them.\n",
    "    for token1 in tokens1:\n",
    "        sim_scores = [model.wv.similarity(token1, token2) for token2 in tokens2]\n",
    "        if sim_scores:\n",
    "            similarities.append(np.mean(sim_scores))\n",
    "    return np.mean(similarities) if similarities else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa5286-f7ee-4052-8a01-4db1d66b2c37",
   "metadata": {},
   "source": [
    "# **Bias Mitigation Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33088203-6da1-491f-94ad-93566364e5d7",
   "metadata": {},
   "source": [
    "### **Bias Mitigation Functions**\n",
    "****This section focuses on mitigating bias by detecting and removing demographic indicators (e.g., names, addresses) from the resume texts. We use spaCy’s Named Entity Recognition (NER) to identify entities such as PERSON and GPE (geopolitical entities) and then remove them from the text. This is a basic debiasing approach that can help reduce the influence of non-skill–related information.****"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656269cf-146d-4a5d-ae38-ad535666bc36",
   "metadata": {},
   "source": [
    "# 4: Bias Mitigation – Removing Demographic Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43d527fc-05b1-4d04-a862-4050442eb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model in spaCy for NER.\n",
    "# (Make sure to install spaCy and the model: python -m spacy download en_core_web_sm)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_demographic_indicators(text):\n",
    "    \"\"\"\n",
    "    Removes demographic indicators such as names and geographical locations from the text.\n",
    "    Uses spaCy's NER to detect entities labeled as PERSON and GPE.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text to clean.\n",
    "    \n",
    "    Returns:\n",
    "        cleaned_text (str): The text after removing demographic indicators.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    # Build a list of tokens that are not demographic indicators.\n",
    "    tokens = [token.text for token in doc if token.ent_type_ not in [\"PERSON\", \"GPE\"]]\n",
    "    # Reconstruct text; simple join may not preserve punctuation perfectly.\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def debiased_text(text):\n",
    "    \"\"\"\n",
    "    Applies bias mitigation techniques to the text.\n",
    "    Currently, it removes demographic indicators.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): Original text.\n",
    "    \n",
    "    Returns:\n",
    "        text (str): Debiased text.\n",
    "    \"\"\"\n",
    "    return remove_demographic_indicators(text)\n",
    "\n",
    "# Example: Clean a sample resume text (uncomment to test)\n",
    "# sample_text = \"John Doe from New York has 5 years of experience in Python.\"\n",
    "# print(debiased_text(sample_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60da019-9aa0-4a12-aabf-a798dafac374",
   "metadata": {},
   "source": [
    "# **Enhanced Feedback System**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ef29e-ba24-475a-88e1-45fde3f05e34",
   "metadata": {},
   "source": [
    "Enhanced Feedback System\n",
    "Here we improve the feedback mechanism to provide detailed, twofold feedback:\n",
    "\n",
    "    *For Recruiters: Explaining why a resume ranks high or low (e.g., matching skills, semantic similarity, and any missing key competencies).\n",
    "\n",
    "    *For Job Seekers: Personalized suggestions for improvement based on missing skills or areas of weakness.\n",
    "\n",
    "We combine insights from both TF‑IDF (for keyword matching) and Word2Vec (for semantic similarity) to create comprehensive feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adebd4d6-143a-4370-9b4c-fd496a6f05ca",
   "metadata": {},
   "source": [
    "# 5: Enhanced Feedback Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7490c99d-3f4f-4c42-b88e-38c7c59637ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def generate_detailed_feedback(job_description, resume, vectorizer, w2v_model):\n",
    "    \"\"\"\n",
    "    Generates detailed feedback for both recruiters and job seekers.\n",
    "    \n",
    "    Parameters:\n",
    "        job_description (str): The job description text.\n",
    "        resume (str): The resume text.\n",
    "        vectorizer (TfidfVectorizer): Pre-fitted TF-IDF vectorizer.\n",
    "        w2v_model (Word2Vec): Trained Word2Vec model.\n",
    "    \n",
    "    Returns:\n",
    "        feedback (dict): Dictionary containing 'Recruiter Feedback' and 'Candidate Feedback'.\n",
    "    \"\"\"\n",
    "    # Transform texts into TF-IDF vectors.\n",
    "    job_tfidf = vectorizer.transform([job_description])\n",
    "    resume_tfidf = vectorizer.transform([resume])\n",
    "    \n",
    "    # Get feature names and extract keywords present in the job description and resume.\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    job_keywords = set([feature_names[i] for i in job_tfidf.indices])\n",
    "    resume_keywords = set([feature_names[i] for i in resume_tfidf.indices])\n",
    "    \n",
    "    # Identify missing skills/keywords.\n",
    "    missing_skills = job_keywords - resume_keywords\n",
    "    matching_skills = job_keywords.intersection(resume_keywords)\n",
    "    \n",
    "    # Calculate semantic similarity using Word2Vec.\n",
    "    semantic_similarity = compute_avg_w2v_similarity(job_description, resume, w2v_model)\n",
    "    \n",
    "    # Build Recruiter Feedback.\n",
    "    recruiter_feedback = (\n",
    "        f\"Semantic Similarity Score: {semantic_similarity:.2f}. \"\n",
    "        f\"Matching Keywords: {', '.join(matching_skills) if matching_skills else 'None'}. \"\n",
    "        f\"Missing Keywords: {', '.join(missing_skills) if missing_skills else 'None'}.\"\n",
    "    )\n",
    "    \n",
    "    # Build Candidate Feedback with suggestions for improvement.\n",
    "    candidate_feedback = (\n",
    "        \"Consider adding details to showcase the following skills/keywords: \"\n",
    "        f\"{', '.join(missing_skills)}.\" if missing_skills else \"Excellent match with the job description!\"\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"Recruiter Feedback\": recruiter_feedback,\n",
    "        \"Candidate Feedback\": candidate_feedback\n",
    "    }\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "# job_desc = \"Looking for a software engineer proficient in Python, machine learning, and data analysis.\"\n",
    "# print(generate_detailed_feedback(job_desc, all_resumes[0], TfidfVectorizer(stop_words=\"english\"), w2v_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb08d36-9b76-43c6-b87a-ceb7c3589975",
   "metadata": {},
   "source": [
    "# **Main Execution – Ranking and Saving Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831e613-a3e5-4def-b086-60bce360c416",
   "metadata": {},
   "source": [
    "***Main Execution: Ranking and Saving Results In this final section, we tie together all conmponents:***\n",
    "\n",
    "* Read and clean resumes from a folder.\n",
    "* Apply bias mitigation to remove demographic details.\n",
    "* Compute TF‑IDF ranking against a given job description.\n",
    "* Generate enhanced feedback using both TF‑IDF and Word2Vec similarity.\n",
    "* Save the ranked resumes along with detailed feedback to a CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9b689-2a5b-4e62-a70f-b6e37c97c19d",
   "metadata": {},
   "source": [
    "# **6: Main Execution Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7cd29d0-7eeb-42bc-bb5d-5e9779147057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to CHATGPT_WORD_EMBERDDING_ranked_resumes_with_enhanced_feedback.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the job description for which resumes will be ranked.\n",
    "job_description = \"\"\"\n",
    "We are seeking a software engineer with expertise in Python, machine learning, and data analysis.\n",
    "The candidate should demonstrate strong problem-solving abilities and hands-on experience with real-world projects.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer (ensure consistency across resume processing)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase=True)\n",
    "\n",
    "# Optionally, apply bias mitigation to all resumes\n",
    "debiased_resumes = [debiased_text(resume) for resume in all_resumes]\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the debiased resumes.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(debiased_resumes)\n",
    "\n",
    "# Transform the job description into a TF-IDF vector.\n",
    "job_tfidf = tfidf_vectorizer.transform([job_description])\n",
    "\n",
    "# Compute cosine similarity between the job description and each resume.\n",
    "similarities = cosine_similarity(job_tfidf, tfidf_matrix)[0]\n",
    "\n",
    "# Rank resumes by similarity score (highest first)\n",
    "ranked_indices = similarities.argsort()[::-1]\n",
    "\n",
    "# Prepare data for saving results.\n",
    "ranked_data = []\n",
    "for idx in ranked_indices:\n",
    "    # Get the original (debiased) resume text.\n",
    "    resume_text = debiased_resumes[idx]\n",
    "    # Generate detailed feedback using our enhanced feedback system.\n",
    "    feedback = generate_detailed_feedback(job_description, resume_text, tfidf_vectorizer, w2v_model)\n",
    "    rank = ranked_indices.tolist().index(idx) + 1\n",
    "    ranked_data.append({\n",
    "        \"Rank\": rank,\n",
    "        \"Resume Index\": idx + 1,\n",
    "        \"Cosine Similarity\": similarities[idx],\n",
    "        \"Recruiter Feedback\": feedback[\"Recruiter Feedback\"],\n",
    "        \"Candidate Feedback\": feedback[\"Candidate Feedback\"],\n",
    "        # Optionally, include a snippet of the resume text.\n",
    "        \"Resume Snippet\": resume_text[:500] + \"...\"\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save to CSV.\n",
    "df = pd.DataFrame(ranked_data)\n",
    "output_csv = \"CHATGPT_WORD_EMBERDDING_ranked_resumes_with_enhanced_feedback.csv\"\n",
    "df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
    "print(f\"Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc34389-1d3e-4cf1-bee1-43ae7d7ff927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Resume Index</th>\n",
       "      <th>Cosine Similarity</th>\n",
       "      <th>Recruiter Feedback</th>\n",
       "      <th>Candidate Feedback</th>\n",
       "      <th>Resume Snippet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1465</td>\n",
       "      <td>0.145224</td>\n",
       "      <td>Semantic Similarity Score: 0.11. Matching Keyw...</td>\n",
       "      <td>Consider adding details to showcase the follow...</td>\n",
       "      <td>ENGINEERING AND QUALITY TECHNICIAN \\n Career O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1935</td>\n",
       "      <td>0.130395</td>\n",
       "      <td>Semantic Similarity Score: 0.09. Matching Keyw...</td>\n",
       "      <td>Consider adding details to showcase the follow...</td>\n",
       "      <td>HR REPRESENTATIVE \\n Summary \\n A motivated bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "      <td>0.124345</td>\n",
       "      <td>Semantic Similarity Score: 0.10. Matching Keyw...</td>\n",
       "      <td>Consider adding details to showcase the follow...</td>\n",
       "      <td>DATA ANALYST \\n Professional Summary \\n Indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>517</td>\n",
       "      <td>0.124105</td>\n",
       "      <td>Semantic Similarity Score: 0.11. Matching Keyw...</td>\n",
       "      <td>Consider adding details to showcase the follow...</td>\n",
       "      <td>Highlights \\n Prog . Languages : C ( 5 + yrs )...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1178</td>\n",
       "      <td>0.117274</td>\n",
       "      <td>Semantic Similarity Score: 0.10. Matching Keyw...</td>\n",
       "      <td>Consider adding details to showcase the follow...</td>\n",
       "      <td>\\n Summary \\n Customer - oriented Principal Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank  Resume Index  Cosine Similarity  \\\n",
       "0     1          1465           0.145224   \n",
       "1     2          1935           0.130395   \n",
       "2     3           513           0.124345   \n",
       "3     4           517           0.124105   \n",
       "4     5          1178           0.117274   \n",
       "\n",
       "                                  Recruiter Feedback  \\\n",
       "0  Semantic Similarity Score: 0.11. Matching Keyw...   \n",
       "1  Semantic Similarity Score: 0.09. Matching Keyw...   \n",
       "2  Semantic Similarity Score: 0.10. Matching Keyw...   \n",
       "3  Semantic Similarity Score: 0.11. Matching Keyw...   \n",
       "4  Semantic Similarity Score: 0.10. Matching Keyw...   \n",
       "\n",
       "                                  Candidate Feedback  \\\n",
       "0  Consider adding details to showcase the follow...   \n",
       "1  Consider adding details to showcase the follow...   \n",
       "2  Consider adding details to showcase the follow...   \n",
       "3  Consider adding details to showcase the follow...   \n",
       "4  Consider adding details to showcase the follow...   \n",
       "\n",
       "                                      Resume Snippet  \n",
       "0  ENGINEERING AND QUALITY TECHNICIAN \\n Career O...  \n",
       "1  HR REPRESENTATIVE \\n Summary \\n A motivated bu...  \n",
       "2  DATA ANALYST \\n Professional Summary \\n Indust...  \n",
       "3  Highlights \\n Prog . Languages : C ( 5 + yrs )...  \n",
       "4  \\n Summary \\n Customer - oriented Principal Co...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99175a0c-f277-457c-8e82-c00d22d06bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Semantic Similarity Score: 0.11. Matching Keywords: python, expertise, real, hands, projects, software, experience, analysis, engineer, data, machine, learning. Missing Keywords: candidate, solving, strong, problem, demonstrate, abilities, world, seeking.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3,3] # Recruiter feedback read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e412b4-2074-40de-9b52-c9a51f88f905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Consider adding details to showcase the following skills/keywords: candidate, solving, strong, problem, demonstrate, abilities, world, seeking.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3,4] # Candidate feedback read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b7d36-f292-4441-ab77-f047700f7a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
